﻿웹 크롤링
	- 웹 크롤러를 이용해서 웹을 탐색해 원하는 정보를 얻어내거나 처리를 수행하는 작업이다.
 	- 웹 페이지 크롤링을 통해 직접 접근해 정보를 수집하거나 자동 이메일 수집 또는 웹 유지관리를 위해 사용한다.
 	- 웹 크롤러 구현 프레임워크 
		- Scrapy, nutch, crawler4j
		- BeautifulSoup, scrapy

BeautifulSoup
	- HTML과 XML에서 정보를 추출할 수 있다.
	- BeautifluSopu는 다운로드 기능을 가지고 있지 않다.
 
Scrapy
	- 웹에 존재하는 정보를 크롤링하기 위한 파이썬 기반의 애플리케이션 프레임워크다.
	- Scrapy 아키텍처
		스케줄러 	
			- 수집 주기 설정
			- 수집주기, 프록시 설정, 멀티 에이전트 설정 기능을 가지고 있다.
			- Scrapy엔진의 수집에 관련된 정책사항을 설정하는 역할 담당
		아이템 파이프라인 		
			- 아이템 파이프라인은 수집할 데이터의 입출력을 담당한다.
			- 수집하려는 항목을 아이템으로 정의하고 수집한 데이터의 형태를 파일 혹은 DBMS로 
			  직접 입력이 가능하도록 설정할 수 있다.
		스파이더 
			- 수집하는 데이터를 크롤링하는 역할을 한다.
			- 스파이더는 스케줄러로부터 프로젝트에서 크롤링하는 정책에 따라 설정값을 요청하여 
			  다운로드로부터 받은 크롤링 데이터를 아이템형태로 아피템 파이프라인으로 전송한다.
		다운로드
			- http, ftp 프로토롤을 해석하여 웹에 있는 데이터를 다운로드 하는 역할을 수행한다.
		
 	- 설치
		필요한 패키지 설치
			$ sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
				- python-dev, zlib1g-dev, libzml2-dev, libxslt1-dev는 lxml이 필요하다
				- libssl-dev, libffi-dev는 cryptography가 필요하다.
		python 3 설치하기
			$ sudo apt-get install python3 python3-dev
		scrapy 설치하기
			$ pip install scrapy
	
		$ pip install beautifulsoup4
		* 다운로드 기능이 제공되지 않으므로 urllib를 추가로 설치한다.
		$ pip install urllib3

	- Scrapy 프로젝트 
		- 프로젝트 생성
			$ scrapy startproject 프로젝트 이름
			$ scrapy startproject oreilly
		- 프로젝트 디렉토리 구조
			oreilly/					// 프로젝트 루트 디렉토리
				scrapy.cfg				// scrapy 프로젝트 설정 파일, 프로젝트 생성시 설정내용이 자동으로 셋팅된다.
				oreilly/				// 프로젝트 메인 디렉토리
					__init__.py			// 파이썬 초기화 파일
					items.py			// 아이템을 설정하는 파이썬 파일
					pipelines.py			// 크롤링한 데이터를 저장하는 방식을 설정하는 파일
					settings.py			// 프로텍트 생성시 스파이더 봇이 생성 관리하는 파일
					spider/				// 스파이더를 저장하는 디렉토리, 스파이더 프로그램을 이 디렉토리 아래에서 작성한다.
						__init__.py		// 파이썬 초기화 파일
		- 아이템 정의하기
			- 아이템은 스크랩한 데이터를 담는 컨테이너다.
			- 아이템은 웹에서 스크랩한 데이터를 저장하기 위한 필드를 선언한다.
			items.py
				import scrapy
				class OreillyItem(scarpy.Item):
					title = scrapy.Field()
					link = scrapy/.Field()
		- 스파이더 프로그램 작성하기
			- 스파이더 프로그램은 크롤링을 담당하는 핵심 부분이다.
			- 스파이더 프로그램의 구성요소
				name : 크롤링 시 사용되는 스파이더를 구분하는 이름을 선언한다.
				       서로 다른 스파이더가 동시에 같은 이름으로 사용될 수 없기 때문에 프롤젝트 내에서 고유한 이름으로 선언해야 한다.
				start_urls : 스파이더가 크롤링을 시작하는 url들의 위치다.
				parse() : 각 url들에 대해서 다운로도딘 응답을 처리하는 메소드다.
			oreillycrawl.py
				from oreilly.items import OreillyItem
				import scrapy
				
				class Oreillyspider(scrapy.Spider):
					name = 'oreilly'
					start_urls = ['http://shop.oreilly.com']
					def parse(self, response):
						div = response.xpath('//div[@class="row"][1]/div/section/div[@class="carousel"]/div[@class="carousel-item"]')
						for d in div:
							item = OreillyItem()
							item['title'] = d.xpath('a/@href').extract()
							item['link'] = d.xpath('div[@class="metadata"]/p/a/text()').extract()
							yield item
		- 데이터 수집 및 저장
			- 프로젝트 루트 디렉토리에 r.csv로 저장된다.
			$ scrapy crawl oreilly -o r.csv
		- 수집된 데이터 예
			title,link
			Agile Data Science 2.0,//shop.oreilly.com/product/xxxxx.do
			Data Science with java,//shop.oreilly.com/product/xxxxx.do
	- Scrapy 명령어
		- Scrapy는 셀 명령어 기반의 프로젝트 구성을 지원한다.
		- 종류
			전역 명령어
				- scarpy startproject [프로젝트명]
					새로운 scrapy 프로젝트를 생성한다.
				- scrapy shell [url]
					파이썬 셀 프롬프트를 구동시킨다.
					지정된 url에 해당하는 웹 컨텐츠를 응답으로 받아서 크롤링은 미리 연습해볼 수 있다.
				- scrapy runspider [스파이더파일명]
					프로젝트 생성없이 독립적으로 스파이더 프로그램을 만들고 구동할 수 있다.
			프로젝트 명령어
				- scrapy crawl [프로젝트명]
					- scrapy crawl oreilly 		
						크롤링한 데이터의 값이 화면에 출력된다.
					- scrapy crawl oreilly -o result.csv
						크롤링한 데이터를 csv형태의 파일로 저장한다.
					- scrapy crawl oreilly -o result.json
						크롤링한 데이터를 json형태의 파일로 저장한다.
				- scrapy check
					스파이더간의 관계를 확인할 때 사용한다.
				- scrapy list
					현재 프로젝트에서 사용가능한 스파이더를 출력한다.
				- scrapy bench
					스파이더의 간단한 벤치마크 테스트를 수행한다.
				- scrapy genspider
					프로젝트내에 스파이더 템플릿을 사용해서 스파이더 프로그램을 작성한다.
					scrapy genspider -d basic
				

			
						
					