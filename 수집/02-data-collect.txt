- 빅데이터의 수집
    - 데이터 수집 계획 수립하기
    - 빅데이터 수집 시스템 구성하기
    - 내/외부 데이터 수집하기
    - 데이터 변환하기
    - 수집 데이터 검증하기

    - 데이터 수집 계획 수립하기
        - 기초 데이터 수집
            - 비즈니스 도에민과 원천 데이터 정보에 대한 이해가 필요한다.
                - 데이터 수집을 위해서 데이터 수집자는 데이터 분석 목표를 이해하고, 비즈니스 도메인에 대한 이해를 바탕으로 원천 데이터를 탐색해야 한다.
                - 데이터 분석에 필요한 대상 원천 데이터의 수집 가능성, 데이터의 보안, 정확성, 수집 난이도, 수집 비용 등의 기초 자료를 수집한다.
        - 기초 데이터 수집 절차
            - 비즈니스 도메인 정보를 수집한다.
                - 도메인 전문가와 인터뷰를 통해서 비즈니스 도메인에 대해 이해한다.
            - 분석 목표 달성을 위한 원천 데이터 선정, 위치, 크기, 유형, 수집 주기를 탐색한다.
                - 수집 데이터 선정
                - 수집 데이터의 위치 탐색 
                    - 내부 데이터인지, 외부 데이터인지 구분해서 수집 방법을 검토한다.
                    - 내부 데이터
                        - 내부 시스템에 원천 데이터가 존재
                        - 수집 기술적 제약이 적다.
                        - 의사소통이 원할하다.
                    - 외부 데이터
                        - 외부 시스템에 원천 데이터가 존재
                        - 수집 기술적 제약이 많다.
                        - 의사소통이 어렵다.
                        - 수집 비용이 발생할 수 있다.
                - 수집 데이터의 유형을 파악하고, 수집 방법을 검토한다.
                    - 수집 데이터의 유형
                        - 정형 데이터
                            - DBtoDB, EAI, ETL
                        - 반정형 데이터
                            - crawler, API, FTP, HTTP
                        - 비정형 데이터
                            - crawler, FTP, HTTP 수집 후 데이터 파싱
                - 수집 비용을 탐색한다.
            - 원천데이터에서 획득할 수 있는 기초 자료를 수집한다.
                - 수집대상, 데이터의 위치, 데이터의 유형, 데이터 수집방법, 데이터의 확보 비용에 대한 체크 리스트를 작성한다.
            - 체크리스트에 따라 기초 데이터를 수집한다.
        - 기초 데이터 세부사항
            - 데이터 유형
                - 정형 데이터
                    - 정형화된 스키마 구조를 갖고 DBMS에 내용이 저장될 수 있는 데이터
                    - DB, File
                - 반정형 데이터
                    - 데이터 내부에 데이터 구조에 대한 메타 정보를 가진 데이터
                    - HTML, XML, JSON, RSS, 웹로그, 센서데이터
                - 비정형 데이터
                    - 수집 데이터 하나하나가 데이터의 객체로 구분될 수 있는 데이터
                    - 이진파일, 동영상, 이미지, 텍스트
            - 데이터의 위치
                - 내부 데이터
                    - 대부분 정형 데이터
                    - 의사소통 원할
                    - 수집 난이도 낮음
                    - 분석가치 보통
                - 외부 데이터
                    - 대부분 반정형 혹은 비정형 데이터
                    - 의사소통 어려움
                    - 추가적인 데이터 가공절차가 필요
                    - 수집난이도 높음
                    - 분석가치 높음
            - 데이터의 저장방식
                - 수집 데이터는 파일 시스템, DBMS 등에 저장할 수 있다.
            - 데이터 수집 기술
                - 정형 데이터 수집 기술
                    - sqoop(Sql to hadoop)
                - 로그 데이터 수집 기술
                    - flume, Scribe, chukwa, ftp
                - 웹 크롤링 및 소셜 데이터 수집 기술
                    - crawler, scrapy, Nutch
                    
    - 빅데이터 수집 시스템 구축하기
        - 정형 데이터 수집 하기
            - 정형 데이터는 데이터베이스의 테이블과 같이 고정된 컬럼에 데이터를 저장하는 구조이거나, 행과 열에 의해 데이터의 속성이 구별된 데이터를 말한다.
            - 정형 데이터를 수집하는 기술에는 주로 apache sqoop를 많이 이용한다.
            - apache sqoop
                - apache sqoop는 apache hadoop과 관계형 데이터베이스간데 대량 데이터를 효과적으로 전송하기 위해 구현된 툴이다.
                - Mysql, Oracle 환경의 데이터를 Hbase, Hive 또는 HDFS로 데이터를 저장할 수 있다.
                - apache sqoop는 모든 적재 과정을 자동화하고 병렬처리 방식으로 작업한다.
                - 특징
                    - 전체 데이터베이스 또는 테이블을 HDFS로 전송가능
                    - 시스템의 사용률과 성능을 고려한 병렬 데이터 전송
                    - RDB에 mapping해서 Hbase와 Hive에 직접 저장 가능
                    - 자바 클래스 생성을 통한 데이터 상호작용 가능
        - 로그/센서 데이터 수집하기
            - 빅데이터 분석에 있어서 대표적인 데이터 유형인 로그/센서 데이터는 주로 apache flume, facebook Scribe, apache chukwa을 사용하여 수집한다.
            - apache flume
                - apache flume은 대용량의 로그 데이터를 효과적으로 수집/집계/이동할 수 있는 안정적이고 신뢰성 있는 분산 서비스를 제공하는 솔루션이다.
                - apache flume은 스트리밍 데이터 흐름에 기반을 둔 간단하고 유연한 구조를 가지고 있다.
                - apache flume은 네트워크 트래픽 데이터, 소셜 미디어 데이터, 이메일 메세지 데이터 등 대량의 이벤트 데이터 전송을 위해 사용할 수 있다.
        - 비정형 혹은 반정형 데이터 수집하기
            - 비정형, 반정형 데이터의 수집의 FTP, HTTP, API, library를 이용하거나, crawler를 이용한다.
            - scrapy
                - scrapy는 웹사이트를 크롤링하고 구조화된 데이터를 수집하는 애플리케이션 프레임워크다.
                - scrapy는 데이터마이닝, 정보처리, 이력 기록과 같은 다양한 애플리케이션에 유용하게 사용할 수 있다.
                - scrapy는 파이썬 기반의 프레임워크로 스크립 과정이 단순하다.
                - scrapy는 한번에 여러 페이지를 불러오기 편하고, scrapyd, scrapinghub 등 부가적인 요소들이 많다.
       
       - apache sqoop 솔루션의 설치 및 구성
            - sqoop 1.4.6 버전을 다운로드 한다.
                - $ wget http://apache.mirror.cdnetworks.com/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
                - $ tar xvfz sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
                - $ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop-1.4.6
                - $ mv sqoop-1.4.6 /usr/local/Cellar
            - sqoop 환경변수 설정하기
                - $ vi ~/.bash_profile
                - export SQOOP_HOME=/usr/local/Cellar/sqoop-1.4.6
                - export PATH=$PATH:$SQOOP_HOME/bin
            - Mysql과 연동을 위해 JDBC드라이버를 다운로드한다.
                - $ wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz
                - $ tar xvfz wget mysql-connector-java-5.1.45.tar.gz
                - $ mv mysql-connector-java-5.1.45-bin.jar $SQOOP_HOME/lib
                
        - apache flume 솔루션 설치 및 구성
            - apache flume 1.8.0 버전을 다운로드 한다.
                - $ wget http://apache.mirror.cdnetworks.com/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz
                - $ tar xvfz apache-flume-1.8.0-bin.tar.gz
                - $ mv apache-flume-1.8.0-bin sqoop-1.4.6 flume-1.8.0
                - $ mv flume-1.8.0 /usr/local/Cellar
            - apache flume 환경변수 설정하기
                - $ vi ~/.bash_profile
                - export FLUME_HOME=/usr/local/Cellar/flume-1.8.0
                - export PATH=$PATH:$FLUME_HOME/bin
                
        - scrapy 솔루션 설치 및 구성
            - Anaconda가 미리 설치되어 있는 경우
                - $ conda install -c conda-forge scrapy
            - Mac에서 설치
                - $ brew install python
                - $ brew update; brew upgrade python
                - $ pip install Scrapy
            - Ubuntu
                - $ sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
                - $ sudo apt-get install python3 python3-dev (python3용 scrapy를 설치하는 경우)
                - $ sudo pip install scrapy
    
    - 내/외부 데이터 수집하기
        - 내/외부 데이터 수집 방법 검토하기
            - 데이터의 위치, 데이터의 종류, 수집주기, 수집방법등을 검토한다.
        - 정형데이터 수집하기
            - apache sqoop 아키텍처
                - apache sqoop는 관계형 데이터베이스에서 읽어온 데이터를 HDFS에 파일 세트로 저장한다.
                - 병렬처리 방식으로 적재하기 때문에 적재한 후에는 HDFS에 여러 개의 파일로 저장된다.
                - apache sqoop는 HDFS에 저장된 파일 셋트를 읽어서 관계형 데이터베이스로 적재하는 것도 가능하다.
                    RDBMS (mysql, oracle, hsqldb) <--------> Sqoop <--------> Hadoop (hdfs, Hive, HBase) 
                    
            - apache sqoop 처리 절차
                - DBMS connector을 사용해서 target DBMS의 특정 테이블의 메타데이터를 가져온다.
                - 특정 테이블의 실제 rows를 가져오는 java 클래스를 생성한다.
                - java class를 이용해서 특정 테이블의 데이터 레코드를 가져오는 Map task를 실행한다.
            
            - apache sqoop 실행하기
                - $ sqoop import						                    // 하둡으로 데이터를 가져올 때는 import를 사용한다.
		            --connect jdbc:mysql://localhost:3306/데이터베이스명        // map이 접근할 데이터베이스 정보를 입력한다.
		            --table  테이블명 						                  // 하둡으로 복사할 테이블을 지정한다.
		            --username 계정명 --password 비밀번호			             // 계정정보를 지정한다.
		            -m 1							                        // 데이터베이스를 하둡에 복사할 때 사용할 맵퍼(mapper)개수를 지정한다.
		            --target-dir 하둡저장경로					                 // 가져온 데이터베이스 내용을 하둡에 저장할 위치를 지정한다.
                
        - 로그/센서 데이터 수집하기
            - apache flume 아키텍처
                - apache flume은 스트리밍 데이터 플로우 기반의 구조며 간단하고 유연한 아키텍처를 가지고 있다.
                - 데이터를 받는 source, 데이터를 저장 또는 전달하는 sink, source와 sink 사이에서 상호 연동을 지원하는 channel로 구성되어 있다.
                - apache flume의 event
                    - flume을 통해 전달되는 데이터의 기본 payload다.
                - apache agent
                    - event를 전달하는 컨테이너다.
                    - source, channel, sink로 흐름을 제엉한다.
                    - agent 사이의 event 이동이 가능하며, 1개의 agent가 다수의 agent로 연결도 가능하다.
                    - source 
                        - 데이터를 가져오는 영역
                        - Avro, Thrift, Exec, JMS, Spooling Dir, Netcat, Sequence Generator
                    - channel
                        - 상호 연동 버퍼
                        - Memory, JDBC, File
                    - sink
                        - channel에서 받은 데이터를 HDFS로 전달
                        - HDFS, Logger, Avro, Thrift, IRC, HBase, File, Roll, ElasticSearch
            - apache flume 처리 절차
                - flume의 작업파일(Config)을 읽어서 source, channel, sink를 설정한다.
                - Config가 로드되면 Source Runner를 수행해서 데이터를 채널로 전달한다.
                - Source가 보낸 Event를 eventQueue에 저장한다.
                - Channel에 있는 Queue에서 메세지를 꺼내서 전달한다.
        
        - 비정형, 반정형 데이터 수집하기
            - scrapy 아키텍처
                - scrapy engine
                    - data flow를 제어한다.
                - Scheduler
                    - 수집 주기를 설정한다.
                - DownLoader
                    - 웹 페이지를 가져온다.
                - Spider
                    - 사용자정의 클래스
                - Item Pipeline
                    - Item
            - scrapy 처리절차
                - engine은 Spider로부터 크롤링할 URL을 가져와서 Scheduler로 스케줄링한다.
                - engine은 Scheduler에게 크롤링할 다음 URL을 요청한다.
                - Scheduler는 크롤링할 다음 URL을 반환하고, Engine은 Downloader에게 URL을 보낸다.
                - 페이지 가져오기가 완료되면, Downloader은 응답을 생성하고, 엔진에게 응답을 보낸다.
                - Engine은 응답을 받고, 응답을 Spider에게 전달한다.
                - Spider는 응답을 프로세싱하고, 스크립된 항목들과 새로운 요청을 Engine에게 반환한다.
                - Engine은 스크립된 항목들을 Item pipelines에 전달하고, Scheduler 처리된 요청을 전달한다.
                - Scheduler로부터 요청이 없을 때 까지 반복한다.
    
        - sqoop를 이용하여 RDBMS dabase의 데이터를 수집하기
            - mysql의 데이터 수집하기
                - mysql DB의 데이터를 hadoop 파일시스템으로 수집한다.
                - 샘플 데이터 준비하기
                    - 샘플 데이터베이스 다운받기
                        - https://github.com/datacharmer/test_db
                    - 샘플 데이터베이스 설치하기
                        - $ unzip test_db-master.zip
                        - $ cd test_db-master/
                        - $ /usr/local/mysql/bin/mysql -u유저아이디 -p -t < employees.sql
                - 
            
                
        

